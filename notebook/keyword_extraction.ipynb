{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import os\n",
    "from collections import Counter\n",
    "from sklearn.decomposition import TruncatedSVD, LatentDirichletAllocation, NMF\n",
    "from gensim.models import HdpModel, CoherenceModel\n",
    "import gensim.corpora as corpora\n",
    "import hdp\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from rake_nltk import Rake\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import yake\n",
    "from keybert import KeyBERT\n",
    "import re\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.max_rows', None)  # Show all rows\n",
    "pd.set_option('display.max_colwidth', None)  # Show full content in each cell\n",
    "pd.set_option('display.width', 1000)  # Set max width\n",
    "\n",
    "# Load spaCy's English model\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # 1. Lowercase and strip leading/trailing whitespace\n",
    "    doc = nlp(text.lower().strip())\n",
    "\n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        # Skip stopwords and punctuation immediately\n",
    "        if token.is_stop or token.is_punct:\n",
    "            continue\n",
    "        \n",
    "        # We want only “pure” ASCII‐letters (if you also want to allow digits, hyphens or underscores, see the alternate regex below)\n",
    "        if re.fullmatch(r\"[A-Za-z]+\", token.text):\n",
    "            tokens.append(token.lemma_)\n",
    "\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "\n",
    "    doc = nlp(text.lower().strip())  # Lowercase and remove whitespace\n",
    "    \n",
    "# Process tokens: lemmatize, filter stopwords/punct/numbers\n",
    "    tokens = [\n",
    "        token.lemma_ \n",
    "        for token in doc \n",
    "        if (\n",
    "            (not token.is_stop) and\n",
    "            (not token.is_punct) and \n",
    "            (token.is_alpha)\n",
    "            # (re.fullmatch(r'[A-Za-z0-9 _-]+', token))\n",
    "        )\n",
    "    ]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
    "    \n",
    "    #use only topn items from vector\n",
    "    sorted_items = sorted_items[:topn]\n",
    "\n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "\n",
    "    for idx, score in sorted_items:\n",
    "        fname = feature_names[idx]\n",
    "        \n",
    "        #keep track of feature name and its corresponding score\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    "\n",
    "    #create a tuples of feature,score\n",
    "    #results = zip(feature_vals,score_vals)\n",
    "    results= {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords(data , cv, tfidf):\n",
    "\n",
    "    feature_names=tfidf.get_feature_names()\n",
    "\n",
    "    #generate tf-idf for the given document\n",
    "    tf_idf_vector=tfidf.transform(data)\n",
    "\n",
    "    #sort the tf-idf vectors by descending order of scores\n",
    "    sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "\n",
    "    #extract only the top n; n here is 10\n",
    "    keywords=extract_topn_from_vector(feature_names,sorted_items,10)\n",
    "    \n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.DataFrame()\n",
    "for i in range(1,3):\n",
    "    q_df = pd.read_csv(os.getcwd() + '/video_data' + str(i) + '.csv')\n",
    "    df = pd.concat([df , q_df])\n",
    "df['processed_title'] = df['title'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2355, 2374)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF vectorization\n",
    "token_pattern = r\"(?u)\\b[a-zA-Z]{3,}\\b\"\n",
    "tf_vectorizer = TfidfVectorizer(\n",
    "    min_df= 2 ,\n",
    "    max_df=0.8 , \n",
    "    ngram_range=(1,2), \n",
    "    use_idf= True, \n",
    "    norm= 'l2', \n",
    "    token_pattern= token_pattern\n",
    "    )\n",
    "tfidf = tf_vectorizer.fit_transform(df['processed_title'])\n",
    "\n",
    "tfidf_df = pd.DataFrame(tfidf.toarray(), columns= tf_vectorizer.get_feature_names_out(), index=df.index)\n",
    "tfidf_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv=CountVectorizer(max_df=0.95,         # ignore words that appear in 95% of documents\n",
    "                   max_features=10000,  # the size of the vocabulary\n",
    "                   ngram_range=(1,3)    # vocabulary contains single words, bigrams, trigrams\n",
    "                  )\n",
    "word_count_vector=cv.fit_transform(df['processed_title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyword Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\n",
    "\n",
    "for title in df['processed_title']:\n",
    "    text += title + \" \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KeyBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                       Coinex: Complete Guide To Leveraged Crypto Trading In 2025\n",
       "1                                  Building a Blockchain in Under 15 Minutes - Programmer explains\n",
       "2    $300M+ cryptocurrency hacks. How they happened and what we've learned. Ivan on Tech explains.\n",
       "3                                            Programmer explains Ethereum | Future of the Internet\n",
       "4                   Bitcoin Hard Fork - Risk of losing Bitcoin in November?  - Programmer explains\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['title'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords:\n",
      "('cryptocurrencie', 0.4062)\n",
      "('bitcoinnew', 0.4031)\n",
      "('hacker', 0.4021)\n",
      "('bitcoin', 0.3831)\n",
      "('cryptocurrency', 0.3723)\n",
      "('cryptozombie', 0.3632)\n",
      "('podcast', 0.3612)\n",
      "('youtubevideo', 0.3585)\n",
      "('cryptocrypto', 0.3578)\n",
      "('brexit', 0.3566)\n"
     ]
    }
   ],
   "source": [
    "from keybert import KeyBERT\n",
    "\n",
    "# Initialize the KeyBERT model\n",
    "model = KeyBERT('distilbert-base-nli-mean-tokens')\n",
    "\n",
    "# Extract keywords\n",
    "keywords = model.extract_keywords(text , top_n= 10)\n",
    "\n",
    "# Print the keywords\n",
    "print(\"Keywords:\")\n",
    "for keyword in keywords:\n",
    "    print(keyword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YAKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = yake.KeywordExtractor(top=10)\n",
    "keywords = [kw[0] for kw in extractor.extract_keywords(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['programmer explain bitcoin', 'bitcoin bitcoin bitcoin', 'week crypto bitcoin', 'programmer explain crypto', 'crypto bull run', 'crypto market analysis', 'bitcoin ethereum crypto', 'live crypto market', 'crypto crypto market', 'week crypto crypto']\n"
     ]
    }
   ],
   "source": [
    "print(keywords)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
