{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import string as st\n",
    "from collections import Counter\n",
    "from sklearn.decomposition import TruncatedSVD, LatentDirichletAllocation, NMF\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.max_rows', None)  # Show all rows\n",
    "pd.set_option('display.max_colwidth', None)  # Show full content in each cell\n",
    "pd.set_option('display.width', 1000)  # Set max width\n",
    "\n",
    "# Load spaCy's English model\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    custom_stopwords = set(nlp.Defaults.stop_words)\n",
    "\n",
    "    doc = nlp(text.lower().strip())  # Lowercase and remove whitespace\n",
    "    \n",
    "# Process tokens: lemmatize, filter stopwords/punct/numbers\n",
    "    tokens = [\n",
    "        token.lemma_ \n",
    "        for token in doc \n",
    "        if (\n",
    "            (not token.is_stop) and\n",
    "            not token.is_punct and token.is_alpha                                  # Remove punctuation\n",
    "            # (token.is_alpha or token.like_num)                       # Keep words/numbers\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keys(topic_matrix):\n",
    "    '''\n",
    "    returns an integer list of predicted topic \n",
    "    categories for a given topic matrix\n",
    "    '''\n",
    "    keys = topic_matrix.argmax(axis=1).tolist()\n",
    "    return keys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keys_to_counts(keys):\n",
    "    '''\n",
    "    returns a tuple of topic categories and their \n",
    "    accompanying magnitudes for a given list of keys\n",
    "    '''\n",
    "    count_pairs = Counter(keys).items()\n",
    "    categories = [pair[0] for pair in count_pairs]\n",
    "    counts = [pair[1] for pair in count_pairs]\n",
    "    return (categories, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_words(n , n_topics, keys, document_term_matrix, count_vectorizer):\n",
    "    '''\n",
    "    returns a list of n_topic strings, where each string contains the n most common \n",
    "    words in a predicted category, in order\n",
    "    '''\n",
    "    top_word_indices = []\n",
    "    for topic in range(n_topics):\n",
    "        temp_vector_sum = 0\n",
    "        for i in range(len(keys)):\n",
    "            if keys[i] == topic:\n",
    "                temp_vector_sum += document_term_matrix[i]\n",
    "        temp_vector_sum = temp_vector_sum.toarray()\n",
    "        top_n_word_indices = np.flip(np.argsort(temp_vector_sum)[0][-n:],0)\n",
    "        top_word_indices.append(top_n_word_indices)   \n",
    "    top_words = []\n",
    "    for topic in top_word_indices:\n",
    "        topic_words = []\n",
    "        for index in topic:\n",
    "            temp_word_vector = np.zeros((1,document_term_matrix.shape[1]))\n",
    "            temp_word_vector[:,index] = 1\n",
    "            the_word = count_vectorizer.inverse_transform(temp_word_vector)[0][0]\n",
    "            topic_words.append(the_word.encode('ascii').decode('utf-8'))\n",
    "        top_words.append(\" \".join(topic_words))         \n",
    "    return top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_topic_vectors(n_topics, keys, two_dim_vectors):\n",
    "    '''\n",
    "    returns a list of centroid vectors from each predicted topic category\n",
    "    '''\n",
    "    mean_topic_vectors = []\n",
    "    for t in range(n_topics):\n",
    "        articles_in_that_topic = []\n",
    "        for i in range(len(keys)):\n",
    "            if keys[i] == t:\n",
    "                articles_in_that_topic.append(two_dim_vectors[i])    \n",
    "        \n",
    "        articles_in_that_topic = np.vstack(articles_in_that_topic)\n",
    "        mean_article_in_that_topic = np.mean(articles_in_that_topic, axis=0)\n",
    "        mean_topic_vectors.append(mean_article_in_that_topic)\n",
    "    return mean_topic_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.DataFrame()\n",
    "for i in range(1,2):\n",
    "    q_df = pd.read_csv(os.getcwd() + '/video_data' + str(i) + '.csv')\n",
    "    df = pd.concat([df , q_df])\n",
    "df['processed_title'] = df['title'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1605, 1581)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF vectorization\n",
    "token_pattern = r\"(?u)\\b[a-zA-Z]{3,}\\b\"\n",
    "vectorizer = TfidfVectorizer(\n",
    "    min_df= 2 ,max_df=0.8 , ngram_range=(1,2), use_idf= True, norm= 'l2', token_pattern= token_pattern\n",
    "                             )\n",
    "tfidf = vectorizer.fit_transform(df['processed_title'])\n",
    "\n",
    "tfidf_df = pd.DataFrame(tfidf.toarray(), columns= vectorizer.get_feature_names_out(), index=df.index)\n",
    "tfidf_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of topics\n",
    "n_topics = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSA\n",
    "\n",
    "This is effectively just a truncated singular value decomposition of a (very high-rank and sparse) document-term matrix, with only the n_topics largest singular values preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa_model = TruncatedSVD(n_components=n_topics)\n",
    "lsa_topic_matrix = lsa_model.fit_transform(tfidf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1:  bitcoin crypto news bullish pump bitcoin crypto explain scam crash break\n",
      "Topic 2:  explain programmer programmer explain ethereum bitcoin hack eos code ethereum programmer lightning\n",
      "Topic 3:  crypto week week crypto news btc blockchain trump bad big crash\n",
      "Topic 4:  buy buy bitcoin bitcoin time sell altcoin rich crypto buy token crypto\n",
      "Topic 5:  bull market run bull run bull market bear coin bear market bitcoin bull cycle\n",
      "Topic 6:  ethereum price bitcoin price price prediction prediction bitcoin bullish interview head start\n",
      "Topic 7:  altcoin memecoin gain millionaire bitcoin altcoin right wealth pump watch gem\n",
      "Topic 8:  moon binance month billion million legend beginning send ralph bitcoin moon\n",
      "Topic 9:  money bank new war collapse ethereum system private civil war civil\n",
      "Topic 10:  trading day strategy trade step trading strategy bybit guide beginner tutorial\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Taking the  argmax of each headline in this topic matrix will give the predicted topics of each headline in the sample. \n",
    "We can then sort these into counts of each topic.\n",
    "  '''\n",
    "\n",
    "lsa_keys = get_keys(lsa_topic_matrix)\n",
    "lsa_categories, lsa_counts = keys_to_counts(lsa_keys)\n",
    "\n",
    "top_n_words_lsa = get_top_n_words(10, n_topics, lsa_keys, tfidf, vectorizer)\n",
    "\n",
    "for i in range(len(top_n_words_lsa)):\n",
    "    print(\"Topic {}: \".format(i+1), top_n_words_lsa[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA\n",
    " LDA is instead a generative probabilistic process, designed with the specific goal of uncovering latent topic structure in text corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LatentDirichletAllocation(n_components=n_topics, learning_method='online', \n",
    "                                          random_state=0, verbose=0)\n",
    "lda_topic_matrix = lda_model.fit_transform(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1:  explain programmer explain programmer ethereum crypto bitcoin crash trading strategy hack\n",
      "Topic 2:  scam bitcoin repeat rally hodl history warning lightning network lightning warning bitcoin\n",
      "Topic 3:  crypto week bitcoin week crypto watch millionaire btc blockchain time day\n",
      "Topic 4:  bull market bull market run bear bull run bitcoin bitcoin bull crypto bear market\n",
      "Topic 5:  trump bitcoin break binance profit crypto real gold github explain\n",
      "Topic 6:  buy bitcoin buy bitcoin crypto news cycle right bullish altcoin bad\n",
      "Topic 7:  memecoin coin crypto plan trade bitcoin game second proof second short\n",
      "Topic 8:  bitcoin crypto money altcoin moon big news month gain etf\n",
      "Topic 9:  explain programmer programmer explain bitcoin need crypto know wallet minute need know\n",
      "Topic 10:  bitcoin price prediction bitcoin price crypto future price prediction interview pump sell\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Once again, we take the  argmax of each entry in the topic matrix \n",
    "to obtain the predicted topic category for each headline. \n",
    "These topic categories can then be characterised by their most frequent words.\n",
    "  '''\n",
    "lda_keys = get_keys(lda_topic_matrix)\n",
    "lda_categories, lda_counts = keys_to_counts(lda_keys)\n",
    "\n",
    "top_n_words_lda = get_top_n_words(10, n_topics, lda_keys, tfidf, vectorizer)\n",
    "\n",
    "for i in range(len(top_n_words_lda)):\n",
    "    print(\"Topic {}: \".format(i+1), top_n_words_lda[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Negative Matrix Factorization (NMF)\n",
    "Non-Negative Matrix Factorization is a statistical method to reduce the dimension of the input corpora. It uses factor analysis method to provide comparatively less weightage to the words with less coherence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_model = NMF(n_components= 'auto')\n",
    "nmf_topic_matrix = nmf_model.fit(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1581, 1581)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf_topic_matrix.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NMF' object has no attribute 'argmax'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m nmf_keys \u001b[38;5;241m=\u001b[39m \u001b[43mget_keys\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnmf_topic_matrix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m nmf_categories, nmf_counts \u001b[38;5;241m=\u001b[39m keys_to_counts(nmf_keys)\n\u001b[1;32m      4\u001b[0m top_n_words_nmf \u001b[38;5;241m=\u001b[39m get_top_n_words(\u001b[38;5;241m10\u001b[39m, n_topics, nmf_keys, tfidf, vectorizer)\n",
      "Cell \u001b[0;32mIn[26], line 6\u001b[0m, in \u001b[0;36mget_keys\u001b[0;34m(topic_matrix)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_keys\u001b[39m(topic_matrix):\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m    returns an integer list of predicted topic \u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m    categories for a given topic matrix\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m     keys \u001b[38;5;241m=\u001b[39m \u001b[43mtopic_matrix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m keys\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NMF' object has no attribute 'argmax'"
     ]
    }
   ],
   "source": [
    "nmf_keys = get_keys(nmf_topic_matrix)\n",
    "nmf_categories, nmf_counts = keys_to_counts(nmf_keys)\n",
    "\n",
    "top_n_words_nmf = get_top_n_words(10, n_topics, nmf_keys, tfidf, vectorizer)\n",
    "\n",
    "for i in range(len(top_n_words_nmf)):\n",
    "    print(\"Topic {}: \".format(i+1), top_n_words_nmf[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_eve",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
